{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregory/miniconda3/envs/Jupyter/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.68578, Val Loss = 13.41380\n",
      "Epoch 2: Train Loss = 0.49313, Val Loss = 18.12076\n",
      "Epoch 3: Train Loss = 0.51417, Val Loss = 19.14291\n",
      "Epoch 4: Train Loss = 0.46469, Val Loss = 18.11123\n",
      "Epoch 5: Train Loss = 0.33173, Val Loss = 15.58720\n",
      "Epoch 6: Train Loss = 0.17867, Val Loss = 11.78655\n",
      "Epoch 7: Train Loss = 0.01061, Val Loss = 7.50494\n",
      "Epoch 8: Train Loss = 0.00177, Val Loss = 3.67135\n",
      "Epoch 9: Train Loss = 0.09410, Val Loss = 5.57468\n",
      "Epoch 10: Train Loss = 0.00707, Val Loss = 7.66018\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "BASE_PATH = os.path.expanduser(\"~/Code/ModelGeo\")\n",
    "DATA_PATH = os.path.join(BASE_PATH, \"data\")\n",
    "if not os.path.exists(DATA_PATH): os.makedirs(DATA_PATH)\n",
    "QUERY_PATH = os.path.join(BASE_PATH, \"queries\")\n",
    "SAVES_PATH = os.path.join(BASE_PATH, \"saves\")\n",
    "if not os.path.exists(SAVES_PATH): os.makedirs(SAVES_PATH)\n",
    "METRICS_FILE = os.path.join(SAVES_PATH, \"all.metrics\")\n",
    "if not os.path.isfile(METRICS_FILE): raise Exception()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "FOLDS = 1\n",
    "SEED = 37\n",
    "VAL_RATIO = 0.2\n",
    "\n",
    "transform = Compose([Resize((224, 224)), ToTensor(), Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "dataset = ImageFolder(DATA_PATH, transform=transform)\n",
    "n = len(dataset)\n",
    "k = len(dataset.classes)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [int(0.7 * n), int(0.15 * n), n - (int(0.7 * n) + int(0.15 * n))])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "class Model(nn.Module):\n",
    "\tdef __init__(self, hidden_dim, num_heads, num_layers, num_classes):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.id = f\"({hidden_dim}-{num_heads}-{num_layers}-{num_classes})\"\n",
    "\t\tself.cnn = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\t\tself.cnn.fc = nn.Identity()\n",
    "\t\tself.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=2048, nhead=num_heads, dim_feedforward=hidden_dim), num_layers=num_layers)\n",
    "\t\tself.fc = nn.Linear(2048, num_classes)\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.cnn(x)\n",
    "\t\tx = x.unsqueeze(1)\n",
    "\t\tx = self.transformer(x)\n",
    "\t\tx = x.squeeze(1)\n",
    "\t\treturn self.fc(x)\n",
    "\n",
    "def save(model, epoch, train_loss, val_loss):\n",
    "\tmodel_path = os.path.join(SAVES_PATH, model.id)\n",
    "\tos.makedirs(model_path, exist_ok=True)\n",
    "\ttorch.save(model.state_dict(), os.path.join(model_path, f\"{model.id}_epoch{epoch}.pth\"))\n",
    "\twith open(os.path.join(model_path, f\"{model.id}_epoch{epoch}.losses\"), \"w\") as f: json.dump({\"train\": train_loss, \"val\": val_loss}, f)\n",
    "\n",
    "def load(model, epoch=None):\n",
    "\tmodel_path = os.path.join(SAVES_PATH, model.id)\n",
    "\tmodel_files = sorted(glob.glob(os.path.join(model_path, \"*.pth\")), key=os.path.getmtime)\n",
    "\tmodel_checkpoint = model_files[-1] if epoch is None else os.path.join(model_path, f\"{model.id}_epoch{epoch}.pth\")\n",
    "\tmodel.load_state_dict(torch.load(model_checkpoint, map_location=DEVICE))\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs=EPOCHS):\n",
    "\tfor epoch in range(1, epochs + 1):\n",
    "\t\tmodel.train()\n",
    "\t\ttrain_loss = 0\n",
    "\t\tfor images, labels in train_loader:\n",
    "\t\t\timages = images.to(DEVICE)\n",
    "\t\t\tlabels = labels.to(DEVICE)\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss = criterion(model(images), labels)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttrain_loss += loss.item()\n",
    "\t\ttrain_loss /= len(train_loader)\n",
    "\n",
    "\t\tmodel.eval()\n",
    "\t\tval_loss = sum(criterion(model(images.to(DEVICE)), labels.to(DEVICE)).item() for images, labels in val_loader) / len(val_loader)\n",
    "\n",
    "\t\tprint(f\"Epoch {epoch}: Train Loss = {train_loss:.5f}, Val Loss = {val_loss:.5f}\")\n",
    "\n",
    "\t\tsave(model, epoch, train_loss, val_loss)\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "\tload(model)\n",
    "\n",
    "\tmodel.eval()\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor images, labels in test_loader:\n",
    "\t\t\timages = images.to(DEVICE)\n",
    "\t\t\tlabels = labels.to(DEVICE)\n",
    "\t\t\t_, predicted = torch.max(model(images), 1)\n",
    "\t\t\tcorrect += (predicted == labels).sum().item()\n",
    "\t\t\ttotal += labels.size(0)\n",
    "\tprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\treturn round(100 * correct / total, 2)\n",
    "\n",
    "def crossvalidate(model, criterion, optimizer, dataset, hyperparameters, epochs=EPOCHS):\n",
    "\tif FOLDS == 1:\n",
    "\t\tsplit = int(0.8 * len(dataset))\n",
    "\t\ttrain_dataset = torch.utils.data.Subset(dataset, range(split))\n",
    "\t\ttest_dataset = torch.utils.data.Subset(dataset, range(split, len(dataset)))\n",
    "\n",
    "\t\tval_size = int(VAL_RATIO * len(train_dataset))\n",
    "\t\ttrain_loader = DataLoader(torch.utils.data.Subset(train_dataset, range(val_size, len(train_dataset))), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\t\tval_loader = DataLoader(torch.utils.data.Subset(train_dataset, range(val_size)), batch_size=BATCH_SIZE)\n",
    "\n",
    "\t\ttrain(model, train_loader, val_loader, criterion, optimizer, epochs=epochs)\n",
    "\t\tM_list = [evaluate(model, DataLoader(test_dataset, batch_size=BATCH_SIZE))]\n",
    "\telse:\n",
    "\t\tkf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "\t\tM_list = []\n",
    "\t\tindices = np.arange(len(dataset))\n",
    "\n",
    "\t\tfor train_i, test_i in kf.split(indices):\n",
    "\t\t\ttrain_dataset = torch.utils.data.Subset(dataset, train_i)\n",
    "\t\t\ttest_dataset = torch.utils.data.Subset(dataset, test_i)\n",
    "\n",
    "\t\t\tval_size = int(VAL_RATIO * len(train_dataset))\n",
    "\t\t\ttrain_loader = DataLoader(torch.utils.data.Subset(train_dataset, range(val_size, len(train_dataset))), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\t\t\tval_loader = DataLoader(torch.utils.data.Subset(train_dataset, range(val_size)), batch_size=BATCH_SIZE)\n",
    "\n",
    "\t\t\ttrain(model, train_loader, val_loader, criterion, optimizer, epochs=epochs)\n",
    "\t\t\tM_list.append(evaluate(model, DataLoader(test_dataset, batch_size=BATCH_SIZE)))\n",
    "\n",
    "\trounded_mean_M = round(np.mean(M_list), 8)\n",
    "\tjson.dump(\n",
    "\t\t(json.load(open(METRICS_FILE)) if os.path.getsize(METRICS_FILE) > 0 else []) + [{\"hyperparameters\": hyperparameters, \"M\": rounded_mean_M}],\n",
    "\t\topen(METRICS_FILE, \"w\"),\n",
    "\t\tindent=4\n",
    "\t)\n",
    "\n",
    "def search(grid):\n",
    "\tfor column in itertools.product(grid[\"hidden_dim\"], grid[\"num_heads\"], grid[\"num_layers\"]):\n",
    "\t\tmodel = Model(*column, k).to(DEVICE)\n",
    "\t\tcriterion = nn.CrossEntropyLoss()\n",
    "\t\toptimizer = optim.Adam(model.parameters(), lr=grid[\"lr\"][0])\n",
    "\n",
    "\t\thyperparameters = f\"{model.id}-{grid['lr'][0]}\"\n",
    "\t\tcrossvalidate(model, criterion, optimizer, dataset, hyperparameters)\n",
    "\n",
    "def predict(model):\n",
    "\tload(model)\n",
    "\n",
    "\tmodel.eval()\n",
    "\tfor image_path in glob.glob(os.path.join(QUERY_PATH, \"*.jpg\")):\n",
    "\t\timage = Image.open(image_path).convert(\"RGB\")\n",
    "\t\ttensor_image = transform(image).to(DEVICE).unsqueeze(0)\n",
    "\t\twith torch.no_grad(): probabilities = torch.softmax(model(tensor_image), dim=1)\n",
    "\t\tpredicted_class = probabilities.argmax().item()\n",
    "\t\tprint(f\"{image_path}: Predicted class = {dataset.classes[predicted_class]}\")\n",
    "\n",
    "grid = {\n",
    "\t\"hidden_dim\": [4096],\n",
    "\t\"num_heads\": [8],\n",
    "\t\"num_layers\": [3],\n",
    "\t\"lr\": [0.0001],\n",
    "}\n",
    "search(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gregory/Code/ModelGeo/queries/aland-1.jpg: Predicted class = Albania\n"
     ]
    }
   ],
   "source": [
    "model = Model(4096, 8, 3, k).to(DEVICE)\n",
    "predict(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
